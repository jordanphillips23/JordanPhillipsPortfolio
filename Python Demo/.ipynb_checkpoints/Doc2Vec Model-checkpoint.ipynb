{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "documents = pd.read_csv(\"cleaned_data.csv\")[\"0\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents)]\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "vec_size = 10\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=20, dm = 1, alpha = alpha, min_alpha=0.00025, workers = 8, min_count = 2, window = 4)\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(i);\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs = model.epochs)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "model.save(\"model2.model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save(open(\"model1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9179', 0.7130836248397827), ('13274', 0.6765183210372925), ('4948', 0.642035186290741), ('2810', 0.626911997795105), ('16602', 0.613538920879364), ('11726', 0.6127225756645203), ('17958', 0.6123162508010864), ('16938', 0.6115821599960327), ('16667', 0.604597806930542), ('11388', 0.5892553329467773)]\n",
      "Roasted Spiced Shrimp Wilted Spinach \n",
      "Tofu  Greens Sun Dried Tomato Strudel Red Pepper Sauce \n",
      "Mussels Sausage  Coconut Milk Lime \n",
      "Crisp Coconut Chicken Roasted Red Bell Pepper Sauce \n",
      "Red  White Blueberry Sundaes \n",
      "Rigatoni Shrimp Tomato Feta Sauce \n",
      "Spicy Salmon Tomatoes Star Anise \n",
      "Cold Curried Carrot Coconut Milk Soup \n",
      "Sausages Green Lentils Tomato Salsa \n",
      "Sensation \n"
     ]
    }
   ],
   "source": [
    "\n",
    "similar_doc = model.docvecs.most_similar([model.infer_vector([\"Chicken\"], epochs=50)])\n",
    "print(similar_doc)\n",
    "for result in similar_doc:\n",
    "   \n",
    "    print(documents[int(result[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
